fulldata_tc %>% filter(variable_name=="ETXY")  ->fulldata_tc
fulldata_tc %>% select(Trial_Id,Exp_Subject_Id,Trial_Nr,timestamp,x,y,t,c,Block_Name,frame_2_onset) -> tc
tc %>% mutate(frame = ifelse(Trial_Id <33,"mb","mv"),
step = ifelse(Trial_Id <9|Trial_Id <41&Trial_Id>32,4,
ifelse(Trial_Id <17&Trial_Id>8|Trial_Id<49&Trial_Id>40,5,
ifelse(Trial_Id <25&Trial_Id>18|Trial_Id <57&Trial_Id>48,6,7)))) -> tc
tc %>%  na.omit()   -> tc
#tc %>% mutate(t=t-117) -> tc # for nasal duration
# NOTE : this used to be 'timestamp - frame_2_onset' but was switched to 't - frame_2_onset'
tc %>% group_by(Exp_Subject_Id,Trial_Nr,Trial_Id)  %>%
mutate(time_norm = t - frame_2_onset) %>% arrange(Exp_Subject_Id,Trial_Nr,Trial_Id,time_norm) %>% mutate(time_norm=time_norm-117) -> tc # for nasal duration -> tc
tc %>% mutate(E_look = ifelse(Block_Name=="exp_E_left"&  # LAXENED WINDOWS BY 30 PX<- uncomment for actual window sizes
x>8&x<208& #x>38&x<178&
y>126&y<326,1,#y>156&y<296,1,
ifelse(Block_Name=="exp_E_right"&
x>594&x<793&   #x>624&x<763&
y>126&y<326, 1,#y>156&y<296,1,
0))) -> tc
options(scipen = 999)
tc %>% mutate(A_look = ifelse(Block_Name=="exp_E_right"&  # LAXENED WINDOWS BY 30 PX<- uncomment for actual window sizes
x>8&x<208& #x>38&x<178&
y>126&y<326,1,#y>156&y<296,1,
ifelse(Block_Name=="exp_E_left"&
x>594&x<793&   #x>624&x<763&
y>126&y<326, 1,#y>156&y<296,1,
0))) -> tc
tc %>% filter(c>0) ->tc ## FILTER BY CONFIDENCE
tc %>% filter(time_norm>0) ->tc ## FILTER BY start of frame 2
tc %>% group_by(Exp_Subject_Id,Trial_Nr,Trial_Id)  %>% mutate(mean_Elook=mean(E_look),
mean_Alook=mean(A_look),
summed_looks =mean_Elook+mean_Alook) -> tc
tc %>% filter(summed_looks!=0) -> tc
tc %>% mutate(time_binrange = cut_width(time_norm,
width=100,dig.lab=10)) -> tc
tc %>% group_by(Exp_Subject_Id,Trial_Nr,Trial_Id,time_binrange)  %>%
mutate(E_look = mean(E_look),
A_look = mean (A_look)) %>% slice(1) ->tc_downsampled
tc_downsampled%>% mutate(time_bin_start=as.numeric(str_extract(time_binrange,"-?[0-9]+"))+50) %>% mutate(time_bin_end = time_bin_start+100) -> tc_downsampled
tc_downsampled %>% mutate(E_pref=E_look-A_look,
A_pref = A_look -E_look) -> tc_downsampled
ggplot(tc,aes(x=time_norm,y=E_look,fill=as.factor(step),color=as.factor(step)))+
xlim(0,2000)+
geom_smooth()
ggplot(tc,aes(x=time_norm,y=E_look,fill=frame,color=frame))+
xlim(0,2000)+
geom_smooth()
ggplot(tc_downsampled,aes(x=time_bin_end,y=E_pref,fill=as.factor(step),color=as.factor(step)))+
xlim(0,2000)+
geom_smooth()
ggplot(tc_downsampled,aes(x=time_bin_end,y=E_pref,fill=frame,color=frame))+
xlim(0,2000)+
geom_smooth()
## NOT DOWNSAMPLED
ggplot(tc,aes(x=time_norm,y=E_look,fill=as.factor(step),color=as.factor(step)))+
xlim(0,4000)+
geom_smooth()
ggplot(tc,aes(x=time_norm,y=E_look,fill=frame,color=frame))+
xlim(0,4000)+
geom_smooth()
## PLOT
library(cowplot)
title1 <- ggdraw() +
draw_label(
"Normalized E preference")
title2 <- ggdraw() +
draw_label(
"Raw looks to E")
title3 <- ggdraw() +
draw_label(
"Categorization")
plot_grid(
title1,
plot_grid(
ggplot(tc_downsampled,aes(x=time_bin_end,y=E_pref,fill=as.factor(step),color=as.factor(step)))+
xlim(0,2000)+
geom_hline(yintercept = 0,lty=2)+
xlab("time (ms)")+
stat_summary(alpha=0.25)+
geom_smooth(),
ggplot(tc_downsampled,aes(x=time_bin_end,y=E_pref,fill=frame,color=frame))+
xlim(0,2000)+
geom_hline(yintercept = 0,lty=2)+
xlab("time (ms)")+
stat_summary(alpha=0.25)+
geom_smooth()),
title2,
plot_grid(
ggplot(tc_downsampled,aes(x=time_bin_end,y=E_look,fill=as.factor(step),color=as.factor(step)))+
xlim(0,2000)+
xlab("time (ms)")+
stat_summary(alpha=0.25)+
geom_smooth(),
ggplot(tc_downsampled,aes(x=time_bin_end,y=E_look,fill=frame,color=frame))+
xlim(0,2000)+
xlab("time (ms)")+
stat_summary(alpha=0.25)+
geom_smooth()
),
title3,
plot_grid(
cat_plot,NULL),
ncol=1, rel_heights = c(0.1, 1,0.1, 1,0.1, 1)) -> results_summary
jpeg(file="summary.jpg",units="in",res=300,height=9,width=9)
results_summary
dev.off()
#creating a unique ID for each vowel
#rawdata_F0 %>% mutate(uniqueID = (ifelse(grepl("VV", Label, fixed = TRUE), str_sub(Label,-3,-1), str_sub(Label,-5,-1)))) -> rawdata_F0
library(tidyverse)
library(hqmisc)
# load in data here
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data<-read.delim("s14_semi_70_250.txt") # change to be your data name
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ###
#### SET THESE PARAMETERS #############################################
# (1) input the sampling rate of your F0 measures in milliseconds
time_step_ms = 1
# (3) name of variable that identifies each unique trajectory
data$uniqueID <- data$Label
# (3) name of variable that identifies time, in milliseconds
data$t_ms <- data$t_ms
# (3) name of variable that identifies speaker, for optional speaker summary stats
data$speaker <- data$Speaker  #just doing one speaker
data$F0 <- data$strF0  #just doing one speaker
data<-data %>%
mutate(Semi = f2st(F0, base=50 ))
data$F0_semitones <- data$Semi
#### ADJUST THESE THRESHOLDS IF DESIRED ####
# from Sundberg (1973), for 10 ms temporal intervals and female speakers
rise_threshold = 1.2631578947
fall_threshold = 1.7142857143
time_mutation = time_step_ms/10
data %>% group_by(uniqueID) %>%
mutate(lead_F0_semitones= lead(F0_semitones, order_by=t_ms),
diff = lead_F0_semitones-F0_semitones,
err = ifelse(diff>0&(abs(diff)*time_mutation)>rise_threshold,1,
ifelse(diff<0&(abs(diff)*time_mutation)>fall_threshold,1,0)),
err_prop_by_ID = mean(err,na.rm = T),
err_count_by_ID = sum(err,na.rm = T),
err_in_ID=ifelse(err_prop_by_ID>0,1,0)) -> data_annotated
data_annotated %>% group_by(uniqueID) %>%
mutate(max_diff = max(diff,na.rm = T)) %>% slice(1) -> data_diffs
data_annotated %>%
group_by(uniqueID) %>%
select(uniqueID,err_prop_by_ID,err_count_by_ID,err_in_ID) %>% slice(1) -> data_summary_by_ID
mean(data_summary_by_ID$err_in_ID)
data_summary_by_ID %>% filter(err_in_ID==1) %>%
select(-err_in_ID) ->data_summary_by_ID_errors_only
data_annotated %>% group_by(speaker) %>%
mutate(err_prop_by_speaker = mean(err,na.rm = T),
err_count_by_speaker = sum(err,na.rm = T)) %>%
select(speaker,err_count_by_speaker,err_prop_by_speaker) %>% slice(1) -> data_summary_by_speaker
View(data_diffs)
# load in data here
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data<-read.delim("s14_semi_70_250.txt") # change to be your data name
# load in data here
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data<-read.delim("s14_semi_70_250.txt") # change to be your data name
View(data)
time_mutation = time_step_ms/10
data %>% group_by(uniqueID) %>%
mutate(lead_F0_semitones= lead(F0_semitones, order_by=t_ms),
diff = lead_F0_semitones-F0_semitones,
err = ifelse(diff>0&(abs(diff)*time_mutation)>rise_threshold,1,
ifelse(diff<0&(abs(diff)*time_mutation)>fall_threshold,1,0)),
err_prop_by_ID = mean(err,na.rm = T),
err_count_by_ID = sum(err,na.rm = T),
err_in_ID=ifelse(err_prop_by_ID>0,1,0)) -> data_annotated
## TRY ten ms
## or look at diffs
#creating a unique ID for each vowel
#rawdata_F0 %>% mutate(uniqueID = (ifelse(grepl("VV", Label, fixed = TRUE), str_sub(Label,-3,-1), str_sub(Label,-5,-1)))) -> rawdata_F0
library(tidyverse)
library(hqmisc)
# semi tone stuff, how to do this w/o normtime?
#write.csv(rawdata_F0, "~/Documents/UW_Madison/IRB Mixtec 2021/Mixtec_Recordings_April22/speaker14/semitone_s14.csv", row.names = FALSE)
# this package required:
# if not installed, use the following: install.packages("tidyverse")
# load in data here
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data<-read.delim("s14_semi_70_250.txt") # change to be your data name
#colnames(data)<-c("Speaker", "Label", "t_ms", "F0")
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ###
#### SET THESE PARAMETERS #############################################
# (1) input the sampling rate of your F0 measures in milliseconds
time_step_ms = 1
## variable names
# input the names of the required variables in your data frame
# (2) name of F0 (in semitones): replace YOUR_VARIABLE_NAME_HERE with your variable
# (3) name of variable that identifies each unique trajectory
data$uniqueID %>%
data$uniqueID <- data$Label
# (3) name of variable that identifies time, in milliseconds
data$t_ms <- data$t_ms
# (3) name of variable that identifies speaker, for optional speaker summary stats
data$speaker <- data$Speaker  #just doing one speaker
data$F0 <- data$strF0  #just doing one speaker
data<-data %>%
mutate(Semi = f2st(F0, base=50 ))
data$F0_semitones <- data$Semi
#### ADJUST THESE THRESHOLDS IF DESIRED ####
# from Sundberg (1973), for 10 ms temporal intervals and female speakers
rise_threshold = 1.2631578947
fall_threshold = 1.7142857143
#######################################################################
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ###
## run the rest of the scrip to annotate the data frame and  the save the outputs
time_mutation = time_step_ms/10
data %>% group_by(uniqueID) %>%
mutate(lead_F0_semitones= lead(F0_semitones, order_by=t_ms),
diff = lead_F0_semitones-F0_semitones,
err = ifelse(diff>0&(abs(diff)*time_mutation)>rise_threshold,1,
ifelse(diff<0&(abs(diff)*time_mutation)>fall_threshold,1,0)),
err_prop_by_ID = mean(err,na.rm = T),
err_count_by_ID = sum(err,na.rm = T),
err_in_ID=ifelse(err_prop_by_ID>0,1,0)) -> data_annotated
## TRY ten ms
## or look at diffs
#creating a unique ID for each vowel
#rawdata_F0 %>% mutate(uniqueID = (ifelse(grepl("VV", Label, fixed = TRUE), str_sub(Label,-3,-1), str_sub(Label,-5,-1)))) -> rawdata_F0
library(tidyverse)
library(hqmisc)
# semi tone stuff, how to do this w/o normtime?
#write.csv(rawdata_F0, "~/Documents/UW_Madison/IRB Mixtec 2021/Mixtec_Recordings_April22/speaker14/semitone_s14.csv", row.names = FALSE)
# this package required:
# if not installed, use the following: install.packages("tidyverse")
# load in data here
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data<-read.delim("s14_semi_70_250.txt") # change to be your data name
#colnames(data)<-c("Speaker", "Label", "t_ms", "F0")
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ###
#### SET THESE PARAMETERS #############################################
# (1) input the sampling rate of your F0 measures in milliseconds
time_step_ms = 1
## variable names
# input the names of the required variables in your data frame
# (2) name of F0 (in semitones): replace YOUR_VARIABLE_NAME_HERE with your variable
# (3) name of variable that identifies each unique trajectory
data$uniqueID %>%
data$uniqueID <- data$Label
# (3) name of variable that identifies time, in milliseconds
data$t_ms <- data$t_ms
# (3) name of variable that identifies speaker, for optional speaker summary stats
data$speaker <- data$Speaker  #just doing one speaker
data$F0 <- data$strF0  #just doing one speaker
data<-data %>%
mutate(Semi = f2st(F0, base=50 ))
data$F0_semitones <- data$Semi
#### ADJUST THESE THRESHOLDS IF DESIRED ####
# from Sundberg (1973), for 10 ms temporal intervals and female speakers
rise_threshold = 1.2631578947
fall_threshold = 1.7142857143
time_mutation = time_step_ms/10
data %>% group_by(uniqueID) %>%
mutate(lead_F0_semitones= lead(F0_semitones, order_by=t_ms),
diff = lead_F0_semitones-F0_semitones,
err = ifelse(diff>0&(abs(diff)*time_mutation)>rise_threshold,1,
ifelse(diff<0&(abs(diff)*time_mutation)>fall_threshold,1,0)),
err_prop_by_ID = mean(err,na.rm = T),
err_count_by_ID = sum(err,na.rm = T),
err_in_ID=ifelse(err_prop_by_ID>0,1,0)) -> data_annotated
data$uniqueID %>%
data$uniqueID <- data$Label
data$uniqueID <- data$Label
# (3) name of variable that identifies time, in milliseconds
data$t_ms <- data$t_ms
# (3) name of variable that identifies speaker, for optional speaker summary stats
data$speaker <- data$Speaker  #just doing one speaker
data$F0 <- data$strF0  #just doing one speaker
data<-data %>%
mutate(Semi = f2st(F0, base=50 ))
View(data)
data$F0_semitones <- data$Semi
#### ADJUST THESE THRESHOLDS IF DESIRED ####
# from Sundberg (1973), for 10 ms temporal intervals and female speakers
rise_threshold = 1.2631578947
fall_threshold = 1.7142857143
time_mutation = time_step_ms/10
data %>% group_by(uniqueID) %>%
mutate(lead_F0_semitones= lead(F0_semitones, order_by=t_ms),
diff = lead_F0_semitones-F0_semitones,
err = ifelse(diff>0&(abs(diff)*time_mutation)>rise_threshold,1,
ifelse(diff<0&(abs(diff)*time_mutation)>fall_threshold,1,0)),
err_prop_by_ID = mean(err,na.rm = T),
err_count_by_ID = sum(err,na.rm = T),
err_in_ID=ifelse(err_prop_by_ID>0,1,0)) -> data_annotated
data_annotated %>% group_by(uniqueID) %>%
mutate(max_diff = max(diff,na.rm = T)) %>% slice(1) -> data_diffs
data_annotated %>%
group_by(uniqueID) %>%
select(uniqueID,err_prop_by_ID,err_count_by_ID,err_in_ID) %>% slice(1) -> data_summary_by_ID
mean(data_summary_by_ID$err_in_ID)
data_summary_by_ID %>% filter(err_in_ID==1) %>%
select(-err_in_ID) ->data_summary_by_ID_errors_only
View(data_diffs)
install.packages("forecast")
install.packages("forecast")
install.packages("forecast")
install.packages("forecast")
install.packages("forecast")
library(forcast)
library(forcats)
library(forecast)
install.packages("forecast")
install.packages("forecast")
install.packages("forecast")
install.packages('forecast', dependencies = TRUE)
install.packages('forecast', dependencies = TRUE)
install.packages('forecast', dependencies = TRUE)
knitr::opts_chunk$set(echo = F)
library(forecast)
install.packages("forecast")
library(forecast)
install.packages("forecast")
library(forecast)
install.packages("fracdiff")
data %>%select(-X) ->data
install.packages("TTR")
install.packages("TTR")
knitr::opts_chunk$set(echo = F)
library(forecast)
library(ggplot2)
library(tidyverse)
library(tidyverse)
library(forecast)
library(cowplot)
data<-read.csv("GAMM_demo_data.csv")
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data<-read.csv("GAMM_demo_data.csv")
data<-read.csv("GAMM_demo_data.csv")
data<-read.csv("GAMM_demo_data.csv")
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data<-read.csv("GAMM_demo_data.csv")
targets<-read.csv("targets.csv")
load("/Users/jeremysteffman/Library/CloudStorage/OneDrive-NorthwesternUniversity/Nuclear Tunes/OSF and shiny apps/OSF_repos/Rising Tunes OSF/models/GAMM_models.RData")
plot_diff(gam_all_tobi_AR1_rt_with_word, view="normtime", comp=list(Group=c("H*HH", "LH*HL")), rm.ranef=T)
library(itsadug)
plot_diff(gam_all_tobi_AR1_rt_with_word, view="normtime", comp=list(Group=c("H*HH", "LH*HL")), rm.ranef=T)
plot_smooth(gam_all_tobi_AR1_rt, view = "normtime", plot_all = "tune",rm.ranef=T)
plot_diff(gam_all_tobi_AR1_rt_with_word, view="normtime", comp=list(tune=c("H*HH", "LH*HL")), rm.ranef=T)
plot_diff(gam_all_tobi_AR1_rt_with_word, view="normtime", comp=list(tune=c("H*HH", "L*HHL")), rm.ranef=T)
plot_diff(gam_all_tobi_AR1_rt_with_word, view="normtime", comp=list(tune=c("H*HH", "L*HHH")), rm.ranef=T)
### REQUIRES THIS PACKAGE ###############################################################
#install.packages("tidyverse") # uncomment and run to install if needed
#########################################################################################
library(tidyverse)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) # set working directory to the folder this script is in-
###  demo data here##
data<-read.csv("demo_data.csv",sep = ",") # change filepath as needed- if you don't run line 14 above
data<-data %>%
group_by(uniqueID) %>%
mutate(lead_F0= lead(F0, order_by=t_ms),
ratio = lead_F0/F0,
halving = ifelse(0.52>ratio&ratio>0.48,1,0), # change 0.52/0.48 to adjust what range of ratios count as halving.  1 in the output means halving has occured.
doubling = ifelse(2.02>ratio&ratio>1.98,1,0), # change 2.02/1.98 to adjust what range of ratios count as doubling. 1 in the output means doubling has occured.
prose =ifelse(halving==1,"halving",ifelse(doubling==1,"doubling","neither")), # this column just says whether halving, doubling, or neither has happened for a sample and the sample that follows it.
halving_mean = mean(halving,na.rm = TRUE), # prop. samples which halve/double
doubling_mean= mean(doubling,na.rm = TRUE),
halving_in_file=ifelse(halving_mean>0,1,0), # 1 if any samples in file halve, 0 otherwise
doubling_in_file=ifelse(doubling_mean>0,1,0))# 1 if any samples in file double, 0 otherwise
setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) # set working directory to the folder this script is in-
###  demo data here##
data<-read.csv("demo_data.csv",sep = ",") # change filepath as needed- if you don't run line 14 above
setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) # set working directory to the folder this script is in-
###  demo data here##
data<-read.csv("demo_data.csv",sep = ",") # change filepath as needed- if you don't run line 14 above
###############################################
# this script detects sudden jumps in F0
# which are likely to be F0 measurement errors.
# created by:  Jeremy Steffman
# last updated: November 3, 2022
##############################################
### required input:
# long format data with the following columns
# -F0 in semitones
# -time in ms (can be at any interval, e.g., 1ms, 10ms, 100ms)
# -a column which uniquely identifies each trajectory
# -an optional column that contains speaker information.
### output
# - an annotated data frame which flags all sample to sample errors
# - a summary of flagged errors by unique identifier
# - a summary of flagged errors by unique identifier ONLY for files which were flagged as errors
# - a summary of errors by speaker, if the speaker column is specified
# this package required:
library(tidyverse)
# if not installed, use the following: install.packages("tidyverse")
# load in data here
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data<-read.csv("example10speakers.csv") # change to be your data
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ###
#### SET THESE PARAMETERS #############################################
# (1) input the sampling rate of your F0 measures in milliseconds
time_step_ms = 10
## variable names
# input the names of the required variables in your data frame
# (2) name of the column containing F0 (in semitones, and Hz): replace YOUR_VARIABLE_NAME_HERE with your variable
data$F0_semitones <- data$F0_semitones
data$F0_Hz<-data$F0_Hz
# (3) name of variable that identifies each unique trajectory
data$uniqueID <- data$uniqueID
# (3) name of variable that identifies time, in milliseconds
data$t_ms <- data$t_ms
# (3) name of variable that identifies speaker, for optional speaker summary statistics
data$speaker <- data$speaker
#### ADJUST THESE THRESHOLDS IF DESIRED ####
# from Sundberg (1973), for 10 ms temporal intervals and female speakers
rise_threshold = 1.2631578947
fall_threshold = 1.7142857143
#######################################################################
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ###
## Run the rest of the scrip to annotate the data frame and  the save the outputs
time_mutation = time_step_ms/10
data %>% group_by(uniqueID) %>%
mutate(lead_F0_semitones= lead(F0_semitones, order_by=t_ms),
lead_F0_Hz= lead(F0_Hz, order_by=t_ms),
diff = lead_F0_semitones-F0_semitones,
ratio_Hz = lead_F0_Hz/F0_Hz,
oct_jump = ifelse(ratio_Hz<0.49|ratio_Hz>1.99,1,0), # halving and doubling ratios for octave jump detection
err = ifelse(diff>0&(abs(diff)*time_mutation)>rise_threshold,1,
ifelse(diff<0&(abs(diff)*time_mutation)>fall_threshold,1,0)),
err_prop_by_ID = mean(err,na.rm = T),
err_count_by_ID = sum(err,na.rm = T),
err_in_ID=ifelse(err_prop_by_ID>0,1,0),
time_of_err = ifelse(err_in_ID==1&err==1,t_ms,0),
F0_of_err = ifelse(lag(err)==1,F0_semitones,0),
# compute octave jumps
oct_jump_prop_by_ID = mean(oct_jump,na.rm = T),
oct_jump_count_by_ID = sum(oct_jump,na.rm = T),
oct_jump_in_ID= ifelse(err_prop_by_ID>0,1,0)) %>%
mutate_if(is.numeric, ~replace(., is.na(.), 0))-> data_annotated
# the following part of the script computes "carryover errors". These are errors for which the sample to sample difference does not necessarily exceed the threshold by the specified amount, but they are within one threshold's worth of the first inaccurate F0 value and are temporally adjacent to it. It is important to emphasize that these *may not be errors*, but  can inspected, for e.g., pitch doubling which raises a whole string of values to be inaccurate.
data_annotated %>% group_by(uniqueID) %>%
mutate(nrow=n(),
carryover_err_start = ifelse(lag(err)==1,1,0),
carryover_err = carryover_err_start) ->data_annotated
x1 =1
repeat {
data_annotated %>% group_by(uniqueID) %>%
mutate(F0_of_err=ifelse(F0_of_err == 0,lag(F0_of_err,order_by = t_ms),F0_of_err)) -> data_annotated
x1 = x1+1
print(paste0(round(x1/max(data_annotated$nrow)*100,0), "% done"))
if (x1 == max(data_annotated$nrow)){
break
}
}
data_annotated$F0_of_err[is.na(data_annotated$F0_of_err)] <- 0
x2=1
repeat {
data_annotated %>% group_by(uniqueID) %>%
mutate(carryover_err =
ifelse(lag(carryover_err)==1&
(lead(F0_semitones)>F0_semitones)&
abs(F0_semitones-F0_of_err)<rise_threshold*1.5|
lag(carryover_err)==1&
(lead(F0_semitones)<F0_semitones)&
abs(F0_semitones-F0_of_err)<fall_threshold*1.5,1,carryover_err))-> data_annotated
x2 = x2+1
print(paste0(round(x2/max(data_annotated$nrow)*100,0), "% done"))
if (x2 == max(data_annotated$nrow)){
break
}
}
data_annotated$carryover_err[is.na(data_annotated$carryover_err)] <- 0
data_annotated %>% group_by(uniqueID) %>%
mutate(mean_carryover_err= mean(carryover_err),
carryover_err = ifelse(t_ms==max(t_ms)&lag(carryover_err)==1,1,carryover_err),
flagged_samples = carryover_err,
# flagged_samples includes the seeding samples for carryover error detection- i.e. some actual errors
carryover_only = ifelse(err==1,0,carryover_err)) -> data_annotated
# carryover_only includes only carry over errors
data_annotated %>%
group_by(uniqueID) %>%
select(uniqueID,err_prop_by_ID,err_count_by_ID,err_in_ID,
oct_jump_prop_by_ID,oct_jump_count_by_ID,oct_jump_in_ID
) %>% slice(1) -> data_summary_by_ID
data_summary_by_ID %>% filter(err_in_ID==1) %>%
select(-err_in_ID) ->data_summary_by_ID_errors_only
data_annotated %>% group_by(speaker) %>%
mutate(err_prop_by_speaker = mean(err,na.rm = T),
err_count_by_speaker = sum(err,na.rm = T)) %>%
select(speaker,err_count_by_speaker,err_prop_by_speaker) %>% slice(1) -> data_summary_by_speaker
# save files as csv - EXISTING FILES WILL BE OVERWRITTEN
write.csv(data_summary_by_speaker,file="output_files/data_summary_by_speaker.csv")
write.csv(data_summary_by_ID,file="output_files/data_summary_by_ID.csv")
write.csv(data_summary_by_ID_errors_only,file="output_files/data_summary_by_ID_errors_only.csv")
write.csv(data_annotated,file="output_files/data_annotated.csv")
## in the OUTPUT the following columns are added.
#
View(data_annotated)
data_annotated %>% select(-nrow) -> data_annotated
data_annotated %>%
group_by(uniqueID) %>%
select(uniqueID,err_prop_by_ID,err_count_by_ID,err_in_ID,
oct_jump_prop_by_ID,oct_jump_count_by_ID,oct_jump_in_ID
) %>% slice(1) -> data_summary_by_ID
data_summary_by_ID %>% filter(err_in_ID==1) %>%
select(-err_in_ID) ->data_summary_by_ID_errors_only
data_annotated %>% group_by(speaker) %>%
mutate(err_prop_by_speaker = mean(err,na.rm = T),
err_count_by_speaker = sum(err,na.rm = T)) %>%
select(speaker,err_count_by_speaker,err_prop_by_speaker) %>% slice(1) -> data_summary_by_speaker
# save files as csv - EXISTING FILES WILL BE OVERWRITTEN
write.csv(data_summary_by_speaker,file="output_files/data_summary_by_speaker.csv")
write.csv(data_summary_by_ID,file="output_files/data_summary_by_ID.csv")
write.csv(data_summary_by_ID_errors_only,file="output_files/data_summary_by_ID_errors_only.csv")
write.csv(data_annotated,file="output_files/data_annotated.csv")
View(data_annotated)
data_annotated %>% select(-nrow,-carryover_err_start) -> data_annotated
View(data_annotated)
View(data_annotated)
